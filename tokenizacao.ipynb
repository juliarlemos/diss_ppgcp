{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiI-i6BOEpKn"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk import FreqDist\n",
        "import re\n",
        "\n",
        "def tokenizar(base, variavel):\n",
        "  \n",
        "  listabase = '\\n'.join(map(str, list(base[variavel]))) #quebra por string da lista\n",
        "  listabase_low = listabase.lower() #colocar tudo em minusculo\n",
        " \n",
        "  #termos para limpar\n",
        "  stopwords = set(nltk.corpus.stopwords.words('portuguese')) #definir as stopwords\n",
        "  excluidas = {'pra', 'vai', 'ter', 'aqui', 'isso', 'cada', 'vou', 'vão',\n",
        "               'fazer', 'dar', 'pode', 'todo', 'porque', 'desde',\n",
        "               'todos', 'quer', 'ainda', 'após', 'ricardo', 'stuckert', 'set', 'ton',\n",
        "               'the', 'nan', 'além', 'ricardostuckert', 'set', 'fez', 'mil', 'sobre', 'milhões',\n",
        "               'assim', 'nesse', 'nos'}\n",
        "  \n",
        "  dicionario_replaces = {'espírito santo':'espírito_santo', 'espirito santo':'espirito_santo','mato grosso do sul':'mato_grosso_do_sul',\n",
        "                         'mato grosso':'mato_grosso', 'rio de janeiro':'rio_de_janeiro', 'rio grande do norte':'rio_grande_do_norte',\n",
        "                         'rio grande do sul':'rio_grande_do_sul', 'santa catarina':'santa_catarina', 'são paulo':'são_paulo',\n",
        "                         'sao paulo':'sao_paulo','distrito federal':'distrito_federal', 'rio branco':'rio_branco', 'são luís':'são_luís',\n",
        "                         'são luis':'são_luis', 'sao luis':'sao_luis', 'sao luís':'sao_luís', 'campo grande':'campo_grande',\n",
        "                         'belo horizonte':'belo_horizonte', 'joão pessoa':'joão_pessoa', 'joao pessoa':'joao_pessoa', 'porto alegre':'porto_alegre',\n",
        "                         'porto velho':'porto_velho', 'boa vista':'boa_vista', 'bom dia':'bom_dia', 'boa tarde':'boa_tarde',\n",
        "                         'boa noite':'boa_noite', 'boa semana':'boa_semana','minas gerais':'minas_gerais'}\n",
        "\n",
        "  #limpeza\n",
        "  for key, value in dicionario_replaces.items():\n",
        "      listabase_low = re.sub(r'\\b{}\\b'.format(key), value, listabase_low)\n",
        "  palavras = [i for i in listabase_low.split() if not i in stopwords] #remover as stopwords\n",
        "  palavras = [i for i in palavras if i  not in excluidas] #remover palavras da lista excluidas\n",
        "  palavras = [i for i in palavras if (i.isnumeric() != True)] #remover números sozinhos\n",
        "  palavras = re.sub(r\"[(http(s)?):\\/\\/(www\\.)?a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\",\"link_excluido\",(\" \".join(palavras))).split() #substituir links por 'link_excluido'\n",
        "  palavras = [i for i in palavras if i != \"link_excluido\"]\n",
        "  \n",
        "  #tokenização\n",
        "  resultado = (\" \".join(palavras)) #juntar todas as palavras que sobraram da limpeza\n",
        "  tokenizer = nltk.RegexpTokenizer(r\"\\w+\") #tokenizar sem pontuação (, . : ; etc), só por word\n",
        "  tokens = tokenizer.tokenize(resultado) #aplicar o tokenizar\n",
        "  tokens = [i for i in tokens if (len(i) > 2)] #só manter tokens com mais de 2 caracteres\n",
        "  tokens = [i for i in tokens if i  not in excluidas] #se tiver com algum elemento que não limpou antes\n",
        "  \n",
        "  return tokens"
      ]
    }
  ]
}