{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uiI-i6BOEpKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5857ddb1-7aeb-4afc-d37b-d54f29ebed91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foi\n",
            "foi\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "'''\n",
        "\n",
        "Código que resulta em compilado_user e tweets_final, respectivamente os dados de perfil e as publicações de uma lista de usuários do Twitter.\n",
        "Necessário acesso de desenvolvedor acadêmico OAuth 2.0.\n",
        "\n",
        "'''\n",
        "\n",
        "def bearer_oauth(r):\n",
        "    r.headers[\"Authorization\"] = f\"Bearer #sua chave de acesso\"\n",
        "    r.headers[\"User-Agent\"] = \"v2UserLookupPython\"\n",
        "    return r\n",
        "\n",
        "userslist = [\"LulaOficial,jairbolsonaro\"] #definição dos usuários a serem estudados\n",
        "\n",
        "####### Coleta de dados dos perfis #######\n",
        "##código original: https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/User-Lookup/get_users_with_bearer_token.py\n",
        "\n",
        "user_fields = \"user.fields=description,created_at,public_metrics\" #definição das métricas coletadas\n",
        "url_list = [] #lista vazia pra popular com as urls que vão ser consultadas\n",
        "\n",
        "for i in userslist: #for pra criar as urls com base na lista de usuários\n",
        "  usernames = \"usernames=\"+i\n",
        "  url = \"https://api.twitter.com/2/users/by?{}&{}\".format(usernames, user_fields)\n",
        "  url_list.append(url)\n",
        "\n",
        "def connect_to_endpoint1(url):\n",
        "    #fazer o request com base na url e na autenticação\n",
        "    response = requests.request(\"GET\", url, auth=bearer_oauth,)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(\n",
        "            \"Request returned an error: {} {}\".format(\n",
        "                response.status_code, response.text\n",
        "            )\n",
        "        )\n",
        "    return response.json()   \n",
        "\n",
        "lista_final = []\n",
        "for i in url_list:\n",
        "    #pra cada url criada com base no nome de usuário, a função de request roda e grava o resultado numa lista\n",
        "    dic_parcial = json.loads((json.dumps(connect_to_endpoint1(i), indent=4, sort_keys=True)))\n",
        "    lista_final.append(dic_parcial) #é criada uma lista de dicionários com a chave 'data\n",
        "\n",
        "compilado_user = []\n",
        "for i in range(len(lista_final)):\n",
        "    #transformação da lista de um dicionário chave 'data' em uma lista de itens de dicionários pra cada perfil separado\n",
        "    compilado_user.extend(lista_final[i]['data'])\n",
        "\n",
        "####### Coleta das publicações #######\n",
        "##código original: https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/User-Tweet-Timeline/user_tweets.py\n",
        "\n",
        "start_time = \"2022-08-16T00:00:00Z\" #data inicial da coleta\n",
        "campos = \"created_at,author_id,lang,public_metrics,possibly_sensitive,entities,context_annotations,source,conversation_id,referenced_tweets,withheld,attachments\" #métricas coletadas\n",
        "metricas = {\"tweet.fields\": campos, \"start_time\":start_time, \"max_results\":\"100\"}\n",
        "\n",
        "def connect_to_endpoint(url, params):\n",
        "   #criar o request de url de ids e parametros dados, usando a autenticação\n",
        "    response = requests.request(\"GET\", url, auth=bearer_oauth, params=params)\n",
        "    #print(response.status_code)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(\n",
        "            \"Request returned an error: {} {}\".format(\n",
        "                response.status_code, response.text\n",
        "            )\n",
        "        )\n",
        "    return response.json()\n",
        "\n",
        "url_id_list = [] #lista vazia pra popular com as urls\n",
        "for i in range(len(compilado_user)): #criar as urls com base na lista de usuários\n",
        "  user_id = (compilado_user[i]['id'])\n",
        "  url = \"https://api.twitter.com/2/users/{}/tweets\".format(user_id)\n",
        "  url_id_list.append(url)\n",
        "\n",
        "tweets_final = []\n",
        "for i in url_id_list:\n",
        "    tweets_parcial = json.loads((json.dumps(connect_to_endpoint(i,metricas), indent=4, sort_keys=True))) #lista parcial porque se bater no limite de 100 tweets precisa rodar a página seguinte\n",
        "    tweets_final.append(tweets_parcial) #incluindo o resultado da coleta da id na lista final\n",
        "    while (tweets_final[len(tweets_final)-1]['meta']['result_count'] == 100): #caso pegue 100 tweets, precisa verificar na página seguinte se tem mais coisa pra coletar\n",
        "      metricas_complemento = {\"tweet.fields\": campos,\"start_time\":start_time, \"max_results\":\"100\", \"pagination_token\":tweets_final[len(tweets_final)-1]['meta']['next_token']} #usa a chave \"next-token\" dada pela api na coleta\n",
        "      tweets_complemento =  json.loads((json.dumps(connect_to_endpoint(i,metricas_complemento), indent=4, sort_keys=True))) #coleta os complementos\n",
        "      tweets_final.append(tweets_complemento) #inclui o resultado dos complementos na lista final\n",
        "    else:\n",
        "      print('foi') #se não tiver mais de 100, avisa só para ter controle visual do código rodando"
      ]
    }
  ]
}