{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiI-i6BOEpKn"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==1.3.4 networkx==2.6.3 pyvis==0.1.9\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk import FreqDist\n",
        "import re\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "\n",
        "base = tt_perfil[tt_perfil['Apelido'] == 'Bolsonaro'] #recorte da base com somente um candidato\n",
        "variavel = 'text' #variável de texto da base\n",
        "nomearquivo = 'arquivo.html'\n",
        "nomecsv = 'arquivo.csv'\n",
        "cor = '#3d70e0'\n",
        "\n",
        "base[variavel] = base[variavel].astype(str)\n",
        "listatokens = []\n",
        "\n",
        "for row in base.index:\n",
        "  #tokenização iterando por cada linha da base de dados para agrupar os n-gramas por post\n",
        "  stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
        "  excluidas = {'pra', 'vai', 'ter', 'aqui', 'isso', 'cada', 'vou', 'vão',\n",
        "               'fazer', 'dar', 'pode', 'todo', 'porque', 'desde', 'boa', 'bom',\n",
        "               'todos', 'quer', 'ainda', 'após', 'ricardo', 'stuckert', 'set', 'ton',\n",
        "               'the', 'nan', 'além', 'ricardostuckert', 'set', 'fez', 'mil', 'sobre', 'milhões',\n",
        "               'assim', 'nesse', 'nos', 'https', 'pro', 'http', 'share_device_id', 'www'}\n",
        "  dicionario_replaces = {'espírito santo':'espírito_santo', 'espirito santo':'espirito_santo', 'mato grosso do sul':'mato_grosso_do_sul', 'mato grosso':'mato_grosso', 'rio de janeiro':'rio_de_janeiro',\n",
        "  'rio grande do norte':'rio_grande_do_norte', 'rio grande do sul':'rio_grande_do_sul', 'santa catarina':'santa_catarina', 'são paulo':'são_paulo', 'sao paulo':'sao_paulo','distrito federal':'distrito_federal',\n",
        "  'rio branco':'rio_branco', 'são luís':'são_luís', 'são luis':'são_luis', 'sao luis':'sao_luis', 'sao luís':'sao_luís', 'campo grande':'campo_grande', 'belo horizonte':'belo_horizonte', 'joão pessoa':'joão_pessoa',\n",
        "  'joao pessoa':'joao_pessoa', 'porto alegre':'porto_alegre', 'porto velho':'porto_velho', 'boa vista':'boa_vista', 'bom dia':'bom_dia', 'boa tarde':'boa_tarde', 'boa noite':'boa_noite', 'boa semana':'boa_semana',\n",
        "  'minas gerais':'minas_gerais'}\n",
        "  base[variavel] = base[variavel].str.lower()\n",
        "  for key, value in dicionario_replaces.items():\n",
        "       base[variavel] = base[variavel].str.replace(key, value)  \n",
        "  tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "  tokenspbi = tokenizer.tokenize(base[variavel][row])\n",
        "  tokenspbi = [i for i in tokenspbi if (len(i) > 2)]\n",
        "  tokenspbi = [i for i in tokenspbi if i  not in excluidas]\n",
        "  tokenspbi = [i for i in tokenspbi if i  not in stopwords]\n",
        "  tokenspbi = [i for i in tokenspbi if (i.isnumeric() != True)]\n",
        "  bigramas = list(nltk.bigrams(tokenspbi)) #para outros n-gramas, usar: list(nltk.ngrams(tokenspbi, n))\n",
        "  listatokens.append(bigramas)\n",
        "\n",
        "#criar df dos bigramas e suas frequencias\n",
        "bigramascorridos = [item for sublist in listatokens for item in sublist]\n",
        "df_bigramas = pd.Series(bigramascorridos).value_counts().to_frame().reset_index()\n",
        "df_bigramas.columns = ['bigrama', 'frequencia']\n",
        "df_bigramas[['b1', 'b2']] = pd.DataFrame(df_bigramas['bigrama'].tolist(), index=df_bigramas.index) #para outros n-gramas, criar n colunas\n",
        "\n",
        "#top tokens que ta no gráfico de frequência\n",
        "base_tokens = tokenizar(base, variavel)\n",
        "frequencia = FreqDist(base_tokens)\n",
        "frequenciasr = pd.Series(dict(frequencia.most_common(30)))\n",
        "top_words = frequenciasr.to_frame().reset_index()\n",
        "top_words.columns = ['token', 'frequencia']\n",
        "toptokens = top_words['token'].tolist()\n",
        "toptokens = '|'.join(map(str, toptokens))\n",
        "\n",
        "#filtrando somente os bigramas que são formados por tokens de top frquencia\n",
        "df_bigramas['b1top'] = df_bigramas['b1'].str.contains(\"(?<!.)(\" + toptokens + \")(?!.)\", flags=re.IGNORECASE)\n",
        "df_bigramas['b2top'] = df_bigramas['b2'].str.contains(\"(?<!.)(\" + toptokens + \")(?!.)\", flags=re.IGNORECASE)\n",
        "df_bigramas['toptokens'] = df_bigramas['b1top'] + df_bigramas['b2top'] #para outros n-gramas, somar n colunas\n",
        "df_bigramas = df_bigramas[df_bigramas['toptokens'] == True]\n",
        "\n",
        "### gráfico\n",
        "\n",
        "#filtrando os bigramas com base em um corte: top 85 e frequência > 1\n",
        "df_bigramas = df_bigramas.head(85)\n",
        "df_bigramasrecorte = df_bigramas[df_bigramas['frequencia'] > 1]\n",
        "\n",
        "d = df_bigramasrecorte.set_index('bigrama').T.to_dict('records')\n",
        "G = nx.Graph()\n",
        "\n",
        "#criando as conexões entre os nós\n",
        "for k, v in d[0].items():\n",
        "    G.add_edge(k[0], k[1], weight= (v))\n",
        "\n",
        "pos = nx.spring_layout(G, k=3)\n",
        "\n",
        "#plotando nodes e edges\n",
        "nx.draw_networkx_nodes(G, pos)\n",
        "nx.draw_networkx_edges(G, pos)\n",
        "\n",
        "#criando a visualização interativa\n",
        "net = Network(height='750px', width='100%', bgcolor='white', font_color='black')\n",
        "\n",
        "for node in G.nodes():\n",
        "    net.add_node(node, title=node, color = cor, size=15)\n",
        "for edge in G.edges():\n",
        "    net.add_edge(edge[0], edge[1], value=G[edge[0]][edge[1]]['weight'])\n",
        "\n",
        "#cor e fonte dos nós\n",
        "for n in net.nodes:\n",
        "    n['font'] = {'size': 20, 'color': '#000000'}\n",
        "\n",
        "net.show(nomearquivo)\n",
        "df_bigramas.to_csv(nomecsv)"
      ]
    }
  ]
}